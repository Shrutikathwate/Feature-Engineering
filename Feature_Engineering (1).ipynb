{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering\n"
      ],
      "metadata": {
        "id": "MkokcZ_sBG7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "   - A parameter is a fixed (constant) numerical value that describes a whole population. OR A parameter is a number that tells us something about everyone in the group, not just a few people.\n",
        "   - Example: The average height of all students in a college → this is a parameter.\n",
        "   - Common population parameters: population mean(μ), population standard deviation(σ), population proportion(p)\n",
        "\n",
        "2. What is correlation? What does negative correlation mean?\n",
        "   - Correlation is a statistical measure that shows how two variables are related and how strongly they move together. Correlation tells us whether one thing changes when another thing changes.\n",
        "   - Example: Height and Weight, Study time and exam marks\n",
        "   - Negative correlation means that when one variable increases, the other decreases. One goes up, the other goes down.\n",
        "   - Example: As speed increases, time to reach a place decreases, As price increases, demand decreases\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "   - Machine Learning (ML) is a branch of Artificial Intelligence that enables computers to learn from data and improve their performance automatically without being explicitly programmed.\n",
        "   - Main Components of Machine Learning: Data, Features, Model, Algorithm, Training Process, Evaluation / Performance Measure, Prediction / Output\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "   - The loss value measures how wrong a model’s predictions are compared to the actual (true) values. Loss tells us the amount of error made by the model.\n",
        "   - Lower loss = better model - A small loss value means predictions are close to actual values. A high loss value means predictions are far from actual values\n",
        "   - Guides model learning - During training, the model tries to minimize the loss, Algorithms like gradient descent adjust model parameters to reduce loss\n",
        "   - Compare models - etween two models trained on the same data: Model with lower loss is generally better\n",
        "   - Detect underfitting and overfitting - High training loss → underfitting\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "   - Continuous variables are variables that can take any value within a range, including decimals. They are measurable quantities.\n",
        "   - Ex: Height (170.5 cm), Weight (62.3 kg)\n",
        "   - Categorical variables are variables that represent categories or groups, not numerical measurements. They describe qualities or labels.\n",
        "   - Ex: Gender (Male, Female), Blood Group (A, B, AB, O), Color (Red, Blue, Green)\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "   - Machine Learning models work with numbers, so categorical variables must be converted into numerical form before training a model.\n",
        "   - Common Techniques to Handle Categorical Variables: Label Encoding, One-Hot Encoding, Ordinal Encoding, Binary Encoding, Frequency / Count Encoding, Target Encoding\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        "   - The training dataset is the portion of data used to teach the model. The model learns patterns, relationships, and parameters from this data. Used to build and fit the model.\n",
        "   - Ex: The model learns how study hours relate to marks.\n",
        "   - The testing dataset is the portion of data used to check how well the model performs on unseen data. The model has not seen this data before. Used to evaluate accuracy and generalization\n",
        "   - Ex: Checking whether the trained model predicts marks correctly for new students.\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "   - sklearn.preprocessing is a module in the scikit-learn (sklearn) library that provides tools to prepare and transform data before applying Machine Learning algorithms.\n",
        "   - It helps clean, scale, encode, and transform raw data into a form that ML models can understand and learn from.\n",
        "\n",
        "9. What is a Test set?\n",
        "   - A test set is a part of the dataset that is used to evaluate the performance of a machine learning model after it has been trained. This data is kept separate from the training data, and the model does not learn from it. The main purpose of the test set is to check how well the model makes predictions on new and unseen data.\n",
        "   - Using a test set helps in understanding the generalization ability of the model. If a model performs well on training data but poorly on the test set, it indicates overfitting. Therefore, the test set provides an unbiased measure of the model’s accuracy and ensures that the model will work well on real-world data.\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "    - In Python, data is commonly split into training and testing sets using the train_test_split function from the sklearn.model_selection module. The training data is used to train the machine learning model, while the testing data is used to evaluate its performance on unseen data. This helps in checking how well the model generalizes.\n",
        "    - Usually, the data is split in ratios like 80% training and 20% testing or 70% training and 30% testing.\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "    - We perform Exploratory Data Analysis (EDA) before fitting a model to understand the data better. EDA helps us identify the structure of the dataset, such as the distribution of variables, relationships between features, and the target variable. It also allows us to detect missing values, outliers, and errors in the data.\n",
        "    - By performing EDA, we can make better decisions about data cleaning, feature selection, and preprocessing. This leads to choosing the right model and improving its performance. Without EDA, the model may learn incorrect patterns, resulting in poor accuracy and unreliable predictions.\n",
        "\n",
        "12. What is correlation?\n",
        "    - Correlation is a statistical measure that shows how two variables are related and how they change with respect to each other.\n",
        "    - It indicates whether an increase or decrease in one variable is associated with an increase or decrease in another variable, helping us understand the strength and direction of their relationship.\n",
        "\n",
        "13. What does negative correlation mean?\n",
        "    - Negative correlation means that when one variable increases, the other variable decreases. In other words, the two variables move in opposite directions.\n",
        "\n",
        "14. How can you find correlation between variables in Python?\n",
        "    - You can find the correlation between variables in Python using Pandas or NumPy.\n",
        "  \n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "    - Causation means that one variable directly causes a change in another variable. In other words, a change in X directly leads to a change in Y.\n",
        "    - Correation: Measures how two variables move together, Can be positive or negative, Observed through data patterns, Ex: Ice cream sales and temperature are positively correlated\n",
        "    - Causation: One variable directly affects another, Always implies a cause-effect relationship, Requires evidence of mechanism or experiment, Ex: Heating water causes it to boil\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "    - An optimizer is an algorithm used in machine learning and deep learning to update the model’s parameters (weights and biases) to minimize the loss function. In simple words, it helps the model learn by reducing errors during training.\n",
        "    - Different Types of Optimizers: Gradient Descent (GD), Momentum, RMSProp (Root Mean Square Propagation), Adam (Adaptive Moment Estimation)\n",
        "\n",
        "17. What is sklearn.linear_model ?\n",
        "    - sklearn.linear_model is a module in scikit-learn that provides linear models for regression and classification.\n",
        "    - These models assume a linear relationship between input features (X) and output (Y).\n",
        "    - It includes algorithms like Linear Regression, Logistic Regression, Ridge, Lasso, etc.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "    - The model.fit() function is used to train a machine learning model on the given data. It learns the relationship between input features (X) and target/output (y) by adjusting the model’s parameters.\n",
        "    - Arguments required: X → Input features (independent variables), y → Target/output (dependent variable)\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "    - The model.predict() function is used to make predictions using a trained machine learning model. After the model has learned from the training data using fit(), predict() uses the model to estimate the target values for new input data.\n",
        "    - Arguments required: X → Input features for which you want predictions (can be test data or new data)\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        "    - Continuous Variables: Take numerical values and can have decimals.  Represent measurable quantities. Examples: Height (170.5 cm), Weight (62.3 kg), Temperature (36.7°C).\n",
        "    - Categorical Variables: Represent categories or groups. Can’t be measured numerically. Examples: Gender (Male/Female), Color (Red/Blue/Green), Blood Group (A/B/AB/O).\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "    - Feature scaling is the process of rescaling numerical features in a dataset so that they have a similar range or scale.\n",
        "    - Common methods: Min-Max Scaling (0–1), Standardization (mean = 0, std = 1)\n",
        "    - Ensures that all features contribute equally to the model. Helps faster and stable convergence in algorithms like Gradient Descent. Improves performance of distance-based models like KNN, SVM, and K-Means.\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "    - Feature scaling in Python is done using the sklearn.preprocessing module, which provides tools like StandardScaler and MinMaxScaler to rescale numerical features. StandardScaler standardizes features by setting the mean to 0 and standard deviation to 1, while MinMaxScaler scales values between 0 and 1. We use the fit_transform() method to fit the scaler on the training data and transform it, ensuring all features are on a similar scale, which helps machine learning models perform better and converge faster.\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "    - sklearn.preprocessing is a module in the scikit-learn library that provides tools to prepare and transform data before applying machine learning models. It is mainly used for tasks like scaling numerical features, normalizing data, encoding categorical variables, and transforming features so that models can learn effectively.\n",
        "    - Example: StandardScaler → standardizes data (mean = 0, std = 1)\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "    - Splitting data for model fitting in Python is done using the train_test_split function from sklearn.model_selection. The dataset is divided into training data, used to train the model, and testing data, used to evaluate its performance on unseen data.\n",
        "\n",
        "25. Explain data encoding?\n",
        "    - Data encoding is the process of converting categorical (non-numerical) data into numerical form so that machine learning models can understand and use it. Most ML algorithms work only with numbers, so encoding is necessary for features like gender, color, or categories.\n",
        "    - Common Encoding Techniques: Label Encoding, One-Hot Encoding, Ordinal Encoding"
      ],
      "metadata": {
        "id": "esDfvEaIBTt5"
      }
    }
  ]
}